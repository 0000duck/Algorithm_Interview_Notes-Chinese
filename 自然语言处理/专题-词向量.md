专题-词向量
===

Index
---
<!-- TOC -->

- [背景知识](#背景知识)
  - [神经语言模型](#神经语言模型)
  - [什么是词向量/词嵌入](#什么是词向量词嵌入)
  - [词向量的理解 TODO](#词向量的理解-todo)
- [Word2Vec](#word2vec)
  - [CBOW 模型](#cbow-模型)
- [GloVe](#glove)
- [FastText](#fasttext)
- [其他实践](#其他实践)
  - [一般 embedding 维度的选择](#一般-embedding-维度的选择)
- [Reference](#reference)

<!-- /TOC -->

# 背景知识
## 神经语言模型
> [神经语言模型](./README.md#神经概率语言模型-nplm)

## 什么是词向量/词嵌入
- 词向量（word embedding）是一个固定长度的实值向量
- 词向量是神经语言模型的**副产品**。
- 词向量是针对“词”提出的。事实上，也可以针对更细或更粗的粒度来进行推广——比如字向量、句向量、文档向量等

## 词向量的理解 TODO
> [word2vec 中的数学原理详解（三）背景知识](https://blog.csdn.net/itplus/article/details/37969817) - CSDN博客
- 在 NLP 任务中，因为机器无法直接理解自然语言，所以首先要做的事情就是将语言**数学化**——词向量就是一种数学化的方式。

**One-hot 表示** 
- TODO

**分布式表示** (distributed representation) 
- 分布式假设
- TODO
- 常见的分布式表示方法
  - 潜在语义分析 (Latent Semantic Analysis, LSA)
    - SVD 分解
  - 隐含狄利克雷分布 (Latent Dirichlet Allocation, LDA)，主题模型
  - 神经网络、深度学习

# Word2Vec
- Word2Vec 本质上也是一个神经语言模型，但是它的目标并不是语言模型本身，而是词向量；因此，其所作的一系列优化，都是为了更快更好的得到词向量
- Word2Vec 提供了两套模型：**CBOW** 和 **Skip-Gram**(SG)
  - CBOW 在已知 `context(w)` 的情况下，预测 `w`
  - SG 在已知 `w` 的情况下预测 `context(w)`
- 从训练集的构建方式可以更好的理解和区别 **CBOW** 和 **SG** 模型
  - 每个训练样本为一个二元组 `(x, y)`，其中 `x`为特征，`y`为标签
  
    假设上下文窗口的大小 `context_window =5`，即 
    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=\{w_{t-2},w_{t-1},{\color{Red}w_t},w_{t&plus;1},w_{t&plus;2}\}"><img src="../assets/公式_20180806115820.png" height="" /></a></div>
    
    或者说 `skip_window = 2`，有 `context_window = skip_window*2 + 1` 
  - CBOW 的训练样本为：
    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=(\{w_{t-2},w_{t-1},w_{t&plus;1},w_{t&plus;2}\},&space;{\color{Red}w_t})"><img src="../assets/公式_20180806120054.png" height="" /></a></div>
    
  - SG 的训练样本为：
    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=({\color{Red}w_t},w_i),\quad&space;i\in\{t-2,&space;t-1,&space;t&plus;1,&space;t&plus;2\}"><img src="../assets/公式_20180806120247.png" height="" /></a></div>

  - 一般来说，
## CBOW 模型
- CBOW 模型的结构与 [N-gram 神经语言模型](./README.md#n-gram-神经语言模型)很相似


# GloVe
TODO

# FastText
TODO: FastText 产生词向量的方式

# 其他实践
## 一般 embedding 维度的选择
> [Feature Columns](https://www.tensorflow.org/versions/master/guide/feature_columns#indicator_and_embedding_columns)  |  TensorFlow 
- 经验公式 `embedding_size = n_categories ** 0.25`
- 在大型语料上训练的词向量维度通常会设置的更大一些，比如 `100~300`
  > 如果根据经验公式，是不需要这么大的，比如 200W 词表的词向量维度只需要 `200W ** 0.25 ≈ 37`

# Reference
- 