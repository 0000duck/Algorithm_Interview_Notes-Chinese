**RoadMap**
---
<!-- TOC -->

- [微博用户画像](#微博用户画像)
  - [任务描述](#任务描述)
  - [思路](#思路)
  - [数据预处理](#数据预处理)
  - [特征工程](#特征工程)
  - [模型](#模型)
- [词向量 TODO](#词向量-todo)
  - [embedding 维度的选择](#embedding-维度的选择)
- [讯飞中文阅读理解评测](#讯飞中文阅读理解评测)
  - [任务说明](#任务说明)
  - [思路](#思路-1)
  - [模型](#模型-1)
    - [Encoder 部分](#encoder-部分)
    - [Decoder 部分](#decoder-部分)
- [CIPS-SOGOU 事实类问答评测](#cips-sogou-事实类问答评测)
  - [任务描述](#任务描述-1)
  - [思路](#思路-2)
  - [模型](#模型-2)
    - [为什么使用 CNN 代替 RNN](#为什么使用-cnn-代替-rnn)
    - [门卷积](#门卷积)

<!-- /TOC -->

# 微博用户画像
> [2017.4-5]; 自然语言处理课程项目
>
> 本来是上届师兄参加的比赛，这次作为我们 NLP 课程的项目

## 任务描述
- 利用给定的新浪微博数据（包括用户个人信息、用户微博文本以及用户粉丝列表，详见数据描述部分），进行微博用户画像，具体包括以下三个任务：
  - 任务1：推断用户的年龄（共3个标签：-1979/1980-1989/1990+）
  - 任务2：推断用户的性别（共2个标签：男/女）
  - 任务3：推断用户的地域（共8个标签：东北/华北/华中/华东/西北/西南/华南/境外）

## 思路
- 形式上是一个多分类问题，**特征工程**是关键
> 当时刚刚接触机器学习，没有这个意识，大部分时间都用来熟悉模型了

## 数据预处理
- **停用词**处理
  - 对词频排序，手动筛选停用词
  <!-- - 只处理最基础的标点和特殊符号，尽量保留词信息 -->
- 对未分词数据用 jieba **分词**
  - 关闭“新词发现”
  - 扩充词典（网络收集，微博语料新词发现+人工筛选）
- 提取**表情符**
  - 默认系列：`[微笑]`等
- **缺失值**处理
  - 根据缺失数据的比例决定填充还是丢弃
- 构建**地域映射表**（任务3）

## 特征工程
- 基本特征
  - 粉丝数量，微博数量，发布平台（来源），发布时间段（离散化），...
- 统计特征
  - 平均每条微博的评论/转发人数，
  - 最大/最小的评论/转发数，
  - 博文中出现的地区数量，
  - 每个时间段的发帖量（每小时，早中晚），...
- **交叉特征**/特征组合
  - 特征组合的种类
    - `[A x A]` —— 对单个特征的值求平方形成的组合特征。
    - `[A X B]` —— 将两个特征的值相乘形成的组合特征。
    - `[A x B x C x ...]` —— 将多个特征的值相乘形成的组合特征。
  - 组合 Ont-Hot 编码
    - 组合两个 `1*3` 的编码可以得到一个 `1*9` 的编码
    - 比如组合**发帖时间**和**发布平台**
    
      `[早, 中, 晚]` x `[手机, 电脑, 平板]` -> `[早+手机, 中+手机, 晚+手机, 早+电脑, ...]`
- 词向量 word2vec
  - [词向量维数的选择](#embedding-维度的选择)

**特征选择**
- 使用 XGBoost 进行特征选择
  ```Python
  params = {...}
  xgtrain = xgb.DMatrix(X, label=y)
  bst = xgb.train(params, xgtrain)
  importance = bst.get_fscore()
  importance = sorted(importance.items(), key=lambda x: x[1])
  importance_df = pd.DataFrame(importance, columns=['feature', 'fscore'])
  importance_df.plot(kind='barh', x='feature', y='fscore')
  ```

## 模型
- **SVM** (baseline)
  > ../机器学习#[支持向量机](../机器学习/README.md#支持向量机)
- **GBDT**(XGBoost库)
  > ../机器学习#[GBDT](../机器学习/README.md#梯度提升决策树-gbdt) & #[XGBoost](../机器学习/README.md#xgboost-算法)


# 词向量 TODO
## embedding 维度的选择
- 经验公式 `embedding_size = n_categories ** 0.25`
- 在大型语料上训练的词向量维度通常会设置的更大一些，比如 `100~300`
  > 如果根据经验公式，是不需要这么大的，比如 200W 词表的词向量维度只需要 `200W ** 0.25 ≈ 37`


# 讯飞中文阅读理解评测
> [2017.7-9]; [第一届“讯飞杯”中文机器阅读理解评测 - CMRC 2017](http://www.hfl-tek.com/cmrc2017)
- 数据集和任务格式是哈工大去年提出的。
  工大去年的论文[`[1607.02250]`](https://arxiv.org/abs/1607.02250)研究了另一个相同形式的数据集。<!-- 是用通用的阅读理解模型做的。 -->

## 任务说明
> [任务介绍 - CMRC 2017](http://www.hfl-tek.com/cmrc2017/task/)
- 以**单个词**为答案的填空类问题、用户提问类问题。答案会在上下文中出现。

- 数据格式
  ```
  篇章：由若干个连续的句子组成的一个文本段落，但文中缺少某一个词，标记为 XXXXX
  问题：缺少的词 XXXXX 所在的句子
  答案：缺少的词 XXXXX
  ```

- 示例
  - 填空类问题
    ```
    1 ||| 工商 协进会 报告 ， 12月 消费者 信心 上升 到 78.1 ， 明显 高于 11月 的 72 。
    2 ||| 另 据 《 华尔街 日报 》 报道 ， 2013年 是 1995年 以来 美国 股市 表现 最 好 的 一 年 。
    3 ||| 这 一 年 里 ， 投资 美国 股市 的 明智 做法 是 追 着 “ 傻钱 ” 跑 。
    4 ||| 所谓 的 “ 傻钱 ” XXXXX ， 其实 就 是 买 入 并 持有 美国 股票 这样 的 普通 组合 。
    5 ||| 这个 策略 要 比 对冲 基金 和 其它 专业 投资者 使用 的 更为 复杂 的 投资 方法 效果 好 得 多 。
    <qid_1> ||| 所谓 的 “ 傻钱 ” XXXXX ， 其实 就 是 买 入 并 持有 美国 股票 这样 的 普通 组合 。
    ```
    答案
    ```
    <qid_1> ||| 策略
    ```
  - 提问类问题
    ```
    1 ||| 工商 协进会 报告 ， 12月 消费者 信心 上升 到 78.1 ， 明显 高于 11月 的 72 。
    2 ||| 另 据 《 华尔街 日报 》 报道 ， 2013年 是 1995年 以来 美国 股市 表现 最 好 的 一 年 。
    3 ||| 这 一 年 里 ， 投资 美国 股市 的 明智 做法 是 追 着 “ 傻钱 ” 跑 。
    4 ||| 所谓 的 “ 傻钱 ” 策略 ， 其实 就 是 买 入 并 持有 美国 股票 这样 的 普通 组合 。
    5 ||| 这个 策略 要 比 对冲 基金 和 其它 专业 投资者 使用 的 更为 复杂 的 投资 方法 效果 好 得 多 。
    <qid_1> ||| 哪 一 年 是 美国 股市 表现 最 好 的 一 年 ？
    ```
    答案
    ```
    <qid_1> ||| 2013年
    ```

## 思路
- 类似语言模型的思路：
  - 语言模型是根据前 n 个词从整个词表中预测下一个词；
  - 这里是根据上下文并从中挑选正确的答案
- 可以采用 **encoder-decoder 框架**，这里相当于 "seq2word"
- 对语言建模来说，LSTM 的效果更好；
  - 为了更好的获取全局的语义信息，可以使用多层 bi-LSTM 对上下文进行编码；

## 模型

### Encoder 部分
- Encoder 部分是一个**多层 bi-LSTM**
  - 因为语料不大，所以层数不多，实际使用的是 3 层 bi-LSTM
- 以答案断开材料，分为上文和下文；两者共享同一个 Encoder，即**参数共享**
  > 为什么共享参数？——模拟人在做完形填空的过程，上下文会共同影响结果。
- 拼接 bi-LSTM 的两段输出向量得到上下文各自的特征向量；然后对上下文的特征向量求平均作为**全局特征**。

### Decoder 部分
- 为了实现在上下文中搜索目标词，而不是在整个词表中：

  控制 Encoder 的输出向量，即全局特征与词向量的**维度相同**，然后计算其与段落中的每个词的 **cos 距离**（即内积），然后通过 softmax 得到所有词的**概率分布**。
- 除了与词向量做内积，还可以使用 LSTM 的状态向量，实验中也有类似的效果，甚至更好（原因未知）

<!-- TODO: 模型图示 -->

<h3>实现细节</h3>

- 原材料已经经过分句和分词处理，实际只是用了**分词**信息。
- 因为每个词有可能多次出现，所以需要对所有相同词的概率求和作为该词的概率。
- 未登录词的处理：所有未登录词会映射到同一个词向量，并开放训练
- 其他词向量在前 n 轮不参与训练，在 n 轮后会加入微调


# CIPS-SOGOU 事实类问答评测
> [2017.10-2018.1]; [CIPS-SOGOU问答比赛](http://task.www.sogou.com/cips-sogou_qa/)
- CIPS-SOGOU问答比赛是由中国中文信息学会（CIPS）和搜狗搜索（SOGOU SEARCH）联合主办的一项开放域的智能问答评测比赛。

## 任务描述
- 针对每个问题 q，给定与之对应的若干候选答案篇章 a1，a2，…，an，要求设计算法从候选篇章中**抽取合适的词语、短语或句子**，形成一段正确、完整、简洁的文本，作为预测答案 apred，目标是 apred 能够正确、完整、简洁地回答问题 q。
- 示例
  ```
  问题: 中国最大的内陆盆地是哪个
  答案：塔里木盆地
  材料：
    1. 中国新疆的塔里木盆地，是世界上最大的内陆盆地，东西长约1500公里，南北最宽处约600公里。盆地底部海拔1000米左右，面积53万平方公里。
    2. 中国最大的固定、半固定沙漠天山与昆仑山之间又有塔里木盆地，面积53万平方公里，是世界最大的内陆盆地。盆地中部是塔克拉玛干大沙漠，面积33.7万平方公里，为世界第二大流动性沙漠。
  ```

## 思路
- 参考论文：[[1607.06275]](https://arxiv.org/abs/1607.06275)——百度利用百度知道等资源构建一个 WebQA 数据集，数据形式与本次评测基本一致；

  论文使用的模型如下：
  <div align="center"><img src="../assets/TIM截图20180719220242.png" height="" /></div>

<h3>结构说明</h3>

- 首先，**对问题编码**，得到问题的**特征向量** `rq`（LSTM + Attention）；
- 然后使用**问题的特征向量**和**材料的词向量**以及其他可选的**人工特征**（trick）**对材料编码**部分，得到材料的特征向量，最后送给 CRF（条件随机场）层得到答案的位置——`B, I, O` 分别表示 Begin、Inside、Outside 答案。
- 值得一提的是，论文用到了两个**人工特征**—— `Question-Evidence common word feature` 和 `Evidence-Evidence common word feature`
  - 前者表示每个在材料中的词是否在问题中出现，后者表示在该材料中出现的词是否在其他材料中出现。
  - 前者出于这一**直觉**——在问题中出现的词往往不是答案的一部分；后者则相反，在不同材料中多次出现的词很可能是答案的一部分。

## 模型
- 本次参与评测的模型也参考了 WebQA 论文结构——问题编码+材料编码+**序列标注**；同时也使用了文中提到的两个特征
- 但是主要结构从 LSTM 改成了 CNN。
- 模型的结构与 WebQA 基本相同
  - 对问题进行编码得到**问题向量**，但是模型中 LSTM 替换为 CNN
  - 将**问题向量**拼接到材料的每一个词向量中；同时加入人工提取的特征
  - 最后的预测任务转化为一个序列标注任务，但是不使用 CRF 而是简单的 "0/1" 标注（CRF不熟悉）

### 为什么使用 CNN 代替 RNN
- RNN 的速度很慢，同时 CNN + Attention 在序列建模任务中越来越强势，所以这次的模型抛弃了 RNN，转向 CNN。
- 从任务本身考虑，我认为也是 CNN 更有利，LSTM 因为能记忆比较长的信息，所以在推断方面有不错的表现（直觉）；但是在事实类问答中，并不需要复杂的推断，答案往往藏在一个 **n-gram 短语**中，而 CNN 能很好的对 n-gram 建模。
- Facebook 提出的**门卷积**在语言建模任务上达到了超越 LSTM 的性能。

### 门卷积
