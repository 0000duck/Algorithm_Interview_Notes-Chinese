Attention机制-论文收录说明
===

Index
---
<!-- TOC -->

- [[2017].Attention_is_All_You_Need](#2017attention_is_all_you_need)
- [[2017].A_Structured_Self-attentive_Sentence_Embedding](#2017a_structured_self-attentive_sentence_embedding)

<!-- /TOC -->

## [2017].Attention_is_All_You_Need


## [2017].A_Structured_Self-attentive_Sentence_Embedding
- [[2017].Attention_is_All_You_Need](#2017attention_is_all_you_need) 中引用了本文
- 本文使用 Self-Attention 学习**任务无关**的 Sentence Embedding
  > 我曾在搜狗的面试中被问到“知道哪些方法可以学习任务无关的句向量吗？”