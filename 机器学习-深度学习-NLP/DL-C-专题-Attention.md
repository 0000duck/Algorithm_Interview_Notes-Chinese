DL-专题-Attention
===

Index
---
<!-- TOC -->

- [Self-Attention](#self-attention)
- [Cross-Attention](#cross-attention)

<!-- /TOC -->

## Self-Attention
- **Self-Attention**（自注意力）是一种将**单个序列内部的不同位置**联系起来的注意力机制，用于计算序列的**表示**。
  > Self-Attention 也称 Intra-Attention（内部注意力）
- Self-Attention 的一些应用：
  - 阅读理解
  - 自动摘要
  - 学习任务无关的 Sentence Embedding
    > ./专题-序列建模/[学习任务无关的 Sentence Embedding](./DL-C-专题-序列建模.md#学习任务无关的-sentence-embedding)


## Cross-Attention