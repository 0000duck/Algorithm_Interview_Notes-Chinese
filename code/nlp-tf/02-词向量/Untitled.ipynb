{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import bz2\n",
    "from collections import Counter, deque\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk # standard preprocessing\n",
    "import operator # sorting items in dictionary by value\n",
    "from math import ceil\n",
    "import csv\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Data size 3360286\n",
      "Example words (start):  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
      "Example words (end):  ['favorable', 'long-term', 'outcomes', 'for', 'around', 'half', 'of', 'those', 'diagnosed', 'with']\n"
     ]
    }
   ],
   "source": [
    "def read_data_small(filename):\n",
    "    \"\"\"\n",
    "    Extract a part of the data\n",
    "    \"\"\"\n",
    "    with bz2.BZ2File(filename) as f:\n",
    "        data = []\n",
    "        file_size = os.stat(filename).st_size\n",
    "        chunk_size = 1024 * 1024  # reading 1 MB at a time as the dataset is moderately large\n",
    "        print('Reading data...')\n",
    "        for i in range(int(ceil(file_size // chunk_size) + 1)):\n",
    "            bytes_to_read = min(chunk_size, file_size - (i * chunk_size))\n",
    "            file_string = f.read(bytes_to_read).decode('utf-8')\n",
    "            file_string = file_string.lower()\n",
    "            # tokenizes a string to words residing in a list\n",
    "            file_string = nltk.word_tokenize(file_string)\n",
    "            data.extend(file_string)\n",
    "    return data\n",
    "\n",
    "filename = \"./data/wikipedia2text-extracted.txt.bz2\"\n",
    "words = read_data_small(filename)\n",
    "print('Data size %d' % len(words))\n",
    "print('Example words (start): ', words[:10])\n",
    "print('Example words (end): ', words[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 69215], ('the', 226881), (',', 184013), ('.', 120944), ('of', 116323)]\n",
      "Sample data [1721, 9, 8, 16471, 223, 4, 5165, 4456, 26, 11590]\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words, vocabulary_size=50000):\n",
    "    word_cnt = [['UNK', -1]]\n",
    "\n",
    "    # Use `Counter` to get the most common words\n",
    "    word_cnt.extend(Counter(words).most_common(vocabulary_size - 1))\n",
    "    word2id = dict()\n",
    "\n",
    "    for word, _ in word_cnt:\n",
    "        word2id[word] = len(word2id)  # ID start from 0\n",
    "\n",
    "    data = list()\n",
    "    unk_cnt = 0\n",
    "    for word in words:\n",
    "        if word in word2id:\n",
    "            index = word2id[word]\n",
    "        else:\n",
    "            index = 0  # word2id['UNK']\n",
    "            unk_cnt += 1\n",
    "        data.append(index)\n",
    "\n",
    "    # word_cnt[0] = ['UNK', unk_cnt]\n",
    "    word_cnt[0][1] = unk_cnt\n",
    "\n",
    "    id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "    assert len(word2id) == vocabulary_size\n",
    "\n",
    "    return data, word_cnt, word2id, id2word\n",
    "\n",
    "data, count, word2id, id2word = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing', 'the', 'opinions', 'or', 'behavior', 'of', 'large']\n",
      "\n",
      "with window_size = 1:\n",
      "    batch: ['is', 'is', 'a', 'a', 'concerted', 'concerted', 'set', 'set']\n",
      "    labels: ['a', 'propaganda', 'concerted', 'is', 'a', 'set', 'concerted', 'of']\n",
      "\n",
      "with window_size = 2:\n",
      "    batch: ['a', 'a', 'a', 'a', 'concerted', 'concerted', 'concerted', 'concerted']\n",
      "    labels: ['concerted', 'propaganda', 'is', 'set', 'is', 'of', 'a', 'set']\n",
      "\n",
      "batch 1:\n",
      "    batch: ['is', 'is', 'a', 'a', 'concerted', 'concerted', 'set', 'set']\n",
      "    labels: ['propaganda', 'a', 'is', 'concerted', 'a', 'set', 'concerted', 'of']\n",
      "\n",
      "batch 2:\n",
      "    batch: ['of', 'of', 'messages', 'messages', 'aimed', 'aimed', 'at', 'at']\n",
      "    labels: ['messages', 'set', 'aimed', 'of', 'at', 'messages', 'influencing', 'aimed']\n",
      "\n",
      "batch 3:\n",
      "    batch: ['influencing', 'influencing', 'the', 'the', 'opinions', 'opinions', 'or', 'or']\n",
      "    labels: ['the', 'at', 'influencing', 'opinions', 'the', 'or', 'opinions', 'behavior']\n"
     ]
    }
   ],
   "source": [
    "span = 2 * skip_window + 1\n",
    "data_index = len(data) - span\n",
    "\"\"\"指定整个滑动窗口 [context center context] 的起始位置\"\"\"\n",
    "\n",
    "def generate_batch_sg(data, batch_size=8, skip_window=1, num_skips=None):\n",
    "    \"\"\"Function to generate **one** training batch for the skip-gram model.\n",
    "\n",
    "    每次完成 batch_size // num_skips 个中心词的构建，每个中心词取 num_skips 个上下文词\n",
    "\n",
    "    Args:\n",
    "        num_skips: 随机选择`num_skips`个窗口中的 context words\n",
    "            在CS224n的课程里没有提到这个参数，也就是默认使用所有的上下文词，即 `num_skips=2 * skip_window`\n",
    "        skip_window: 一侧的窗口长度，完整的窗口大小为 1+2*skip_window\n",
    "\n",
    "    Returns:\n",
    "        inputs: center_words\n",
    "            形如 ndarray([1,1,33,33,55,55，67,67]) 每个中心词的重复次数等于`num_skips`\n",
    "        labels: context_words\n",
    "            形如 ndarray([23,243,543,65,7658,342，8567,3123])\n",
    "            长度与 center_words 相同，分别是对应中心词的上下文词，即 (1,23),(1,243),(33,543),...\n",
    "    \"\"\"\n",
    "    global data_index\n",
    "\n",
    "    if num_skips is None:\n",
    "        num_skips = 2 * skip_window\n",
    "    else:\n",
    "        assert num_skips <= 2 * skip_window\n",
    "    assert batch_size % num_skips == 0\n",
    "\n",
    "    # center_words\n",
    "    inputs = np.ndarray(shape=(batch_size,), dtype=np.int32)  # (batch_size,)\n",
    "    # context_words\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)  # (batch_size, 1)\n",
    "\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = deque(maxlen=span)  # 双端队列，支持自动弹出\n",
    "\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index += 1\n",
    "        \n",
    "    context_words = [w for w in range(span) if w != skip_window]\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = random.sample(context_words, num_skips)\n",
    "\n",
    "        for j, context_word in enumerate(context_words):\n",
    "            inputs[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        \n",
    "        if data_index >= len(data):\n",
    "            print(data_index)\n",
    "            buffer = data[:span]\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "print('data:', [id2word[di] for di in data[:16]])\n",
    "\n",
    "for skip_window in [1, 2]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch_sg(data, batch_size=8, skip_window=skip_window)\n",
    "    print('\\nwith window_size = %d:' % skip_window)\n",
    "    print('    batch:', [id2word[bi] for bi in batch])\n",
    "    print('    labels:', [id2word[li] for li in labels.reshape(8)])\n",
    "\n",
    "data_index = 0\n",
    "for i in range(1, 4):\n",
    "    batch, labels = generate_batch_sg(data, batch_size=8, skip_window=1)\n",
    "#     print(data_index)\n",
    "    print('\\nbatch %d:' % i)\n",
    "    print('    batch:', [id2word[bi] for bi in batch])\n",
    "    print('    labels:', [id2word[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3360286\n"
     ]
    }
   ],
   "source": [
    "data_index = len(data)\n",
    "print(data_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 2000: 3.918839\n",
      "Average loss at step 4000: 3.496119\n",
      "Average loss at step 6000: 3.489038\n",
      "Average loss at step 8000: 3.527437\n",
      "Average loss at step 10000: 3.412748\n",
      "Nearest to 's: and, hold'em, cixi, the, monty, imbalances, this, july,\n",
      "Nearest to and: the, ,, in, to, ., on, with, 's,\n",
      "Nearest to one: viewing, equation, verge, humans, alec, sorter, pre-selection, pinprick,\n",
      "Nearest to be: 20.7, landmasses, truesdell, yezidis, capable, espionage, yams, rica,\n",
      "Nearest to also: baez, rorty, heidelberg, unionpay, misdemeanor, café, incidental, dent,\n",
      "Nearest to ): UNK, natalie, rep., constitutionality, physiographic, conclusively, chichibu, sogdian,\n",
      "Nearest to was: is, give, in, could, ., excl, had, glutamine,\n",
      "Nearest to from: département, loans, 1.00794, growth, removing, vacationers, languish, for,\n",
      "Nearest to by: that, unpredictably, ciws, neighborhoods, arsuf, pot-limit, covered, compel,\n",
      "Nearest to which: that, and, ., tropical, harder, go, pétion, aliases,\n",
      "Nearest to were: are, persuades, numbers, elongated, sooners, saudi, storylines, myoglobin,\n",
      "Nearest to but: beaufort, bang, argued, spermatozoa, shapley, unpopular, nordisk, lebanese,\n",
      "Nearest to other: graz, 6-2., struggling, j., protestors, hectare, asynchronous, tectonic,\n",
      "Nearest to this: an, rapporteur, laboring, 20.5, evaluating, pollack, dietitians, 's,\n",
      "Nearest to he: equus, serve-and-volley, yama, pataliputra, excelled, comoving, it, late,\n",
      "Nearest to to: in, and, ,, ., deals, that, panthers, the,\n",
      "Average loss at step 12000: 3.405444\n",
      "Average loss at step 14000: 3.412812\n",
      "Average loss at step 16000: 3.371524\n",
      "Average loss at step 18000: 3.333519\n",
      "Average loss at step 20000: 3.331651\n",
      "Nearest to 's: his, the, an, by, in, was, ,, debit,\n",
      "Nearest to and: ,, in, the, ., to, with, UNK, of,\n",
      "Nearest to one: barrier, equation, grandis, viewing, rounder, brightest, beltways, sulci,\n",
      "Nearest to be: fundraiser, have, williams, ferried, rosalind, futhark, operatives, tricks,\n",
      "Nearest to also: as, rwandan, morals, handbook, ', vannevar, ttv, '',\n",
      "Nearest to ): UNK, unproblematic, judging, hier, liszt, burgundy, in, seldom,\n",
      "Nearest to was: is, the, 's, had, ,, were, cerebrovascular, jimmie,\n",
      "Nearest to from: in, with, enquiries, on, 1.00794, by, cd-rom, dramatics,\n",
      "Nearest to by: ., ,, in, for, 's, and, annulment, catorce,\n",
      "Nearest to which: transformative, ., testable, pavel, boundary, a, beattie, spanning,\n",
      "Nearest to were: kentish, are, was, albrecht, committed, evolutionism, crooked, deutsch-französische,\n",
      "Nearest to but: minimization, lebanese, disinformation, snap, yusuf, bouts, multi-billion-dollar, kantymir,\n",
      "Nearest to other: graz, asynchronous, antiplatelet, kane, complains, tuna, 340,000, chalk,\n",
      "Nearest to this: an, nationwide, canopy, nassau/stadhouderskade, 's, atmosphere, it, sized,\n",
      "Nearest to he: his, positivists, real-life, it, in, willy, pensiero, brandon,\n",
      "Nearest to to: of, and, ., fat-soluble, the, nestor, in, zur,\n",
      "Average loss at step 22000: 3.345542\n",
      "Average loss at step 24000: 3.314467\n",
      "Average loss at step 26000: 3.293226\n",
      "Average loss at step 28000: 3.292250\n",
      "Average loss at step 30000: 3.308144\n",
      "Nearest to 's: UNK, janković, and, the, his, by, its, '',\n",
      "Nearest to and: ., in, UNK, by, the, half-life, ,, from,\n",
      "Nearest to one: beltways, 1787, viewing, electron-positron, matriarchal, newly-created, carmel, weakening,\n",
      "Nearest to be: have, 1057, i-90, were, they, rubble, differentiable, dirac,\n",
      "Nearest to also: belt-like, as, staunch, escalated, imposition, revenue, used, visually,\n",
      "Nearest to ): UNK, (, 4,400, or, playlists, urbanisation, pensiero, sar,\n",
      "Nearest to was: is, were, fences, minimization, kelapa, peat, left-to-right, became,\n",
      "Nearest to from: and, its, incl, were, with, by, when, loans,\n",
      "Nearest to by: and, after, in, or, for, had, aging, 's,\n",
      "Nearest to which: but, that, brunt, and, chin, hippodrome, istum, 1937.,\n",
      "Nearest to were: are, was, had, from, be, devatas, list, pows,\n",
      "Nearest to but: which, when, and, single-member, only, however, minimization, hula,\n",
      "Nearest to other: academic, knorozov, graz, 340,000, politics, rage, kane, hypsilophodontids,\n",
      "Nearest to this: a, it, that, contenders, treatment-resistant, conversely, he, built-in,\n",
      "Nearest to he: it, 75,000, that, a, who, first-generation, 20.5, ellery,\n",
      "Nearest to to: throwback, and, longipes, estienne, carnaval, would, five-pointed, deals,\n",
      "Average loss at step 32000: 3.291064\n",
      "Average loss at step 34000: 3.297983\n",
      "Average loss at step 36000: 3.239163\n",
      "Average loss at step 38000: 3.286847\n",
      "Average loss at step 40000: 3.279999\n",
      "Nearest to 's: the, its, and, of, malthus, s., airborne, disraeli,\n",
      "Nearest to and: ,, the, in, ., from, with, of, which,\n",
      "Nearest to one: viewing, titel, glides, occupy, marsden, enemies, fervently, grouped,\n",
      "Nearest to be: it, communal, 239, have, samarkand, johanneum, rubble, accentuating,\n",
      "Nearest to also: livonia, suing, lepanto, consulate, correlates, sinks, banebdjed, enlargement,\n",
      "Nearest to ): UNK, (, subcity, reactivating, chronically, equating, buridan, 1025,\n",
      "Nearest to was: is, are, has, became, thirst, displays, italian, polanski,\n",
      "Nearest to from: in, and, to, of, the, by, ., cycads,\n",
      "Nearest to by: from, which, security, of, promulgation, discussed, who, for,\n",
      "Nearest to which: but, ., it, and, counterproductive, a, by, trustee,\n",
      "Nearest to were: are, many, other, 500., thousand, all, have, abuja,\n",
      "Nearest to but: which, skip, ., though, reyes, 1917, macedonians, mime,\n",
      "Nearest to other: many, were, pubis, simpson, these, kane, quips, discordianism,\n",
      "Nearest to this: it, its, a, an, rené, aikikai, vector, scrooge,\n",
      "Nearest to he: his, she, absolutist, step, mckay, gta, falklands, it,\n",
      "Nearest to to: from, ., of, and, in, that, 'man, wernher,\n",
      "Average loss at step 42000: 3.316789\n",
      "Average loss at step 44000: 3.287293\n",
      "Average loss at step 46000: 3.311136\n",
      "Average loss at step 48000: 3.293812\n",
      "Average loss at step 50000: 3.277667\n",
      "Nearest to 's: his, the, after, was, planner, its, commercialized, modular,\n",
      "Nearest to and: to, ,, the, of, enticing, with, in, a,\n",
      "Nearest to one: two, what, duan, each, postage, predetermined, occupy, same,\n",
      "Nearest to be: have, montenegrins, pots, if, communal, rewrites, intimidating, utilised,\n",
      "Nearest to also: 1658, fitzgerald, livonia, still, convalesced, georgian-style, maritime, used,\n",
      "Nearest to ): UNK, bangla, ft, asbury, neumann, imagery, high-risk, secretaries,\n",
      "Nearest to was: ethnographic, is, his, were, had, became, 's, after,\n",
      "Nearest to from: between, lose, 1904, into, baziak, in, ;, dublin,\n",
      "Nearest to by: fauvism, ender, who, forlorn, wa, jiā, rois, interdependence,\n",
      "Nearest to which: but, they, only, annihilation, consecrated, kansas, circumference, augustine,\n",
      "Nearest to were: are, was, had, acc, their, many, preceded, all,\n",
      "Nearest to but: which, though, however, not, they, although, widely, still,\n",
      "Nearest to other: many, pubis, are, kane, graz, n-terminus, such, their,\n",
      "Nearest to this: it, a, however, hathigumpha, these, impractical, based, ricans,\n",
      "Nearest to he: his, she, gta, émile, differs, who, 18, compete,\n",
      "Nearest to to: and, olaf, ,, estienne, illusion, armorica, that, oppenheimer-phillips,\n",
      "Average loss at step 52000: 3.233692\n",
      "Average loss at step 54000: 3.261050\n",
      "Average loss at step 56000: 3.271396\n",
      "Average loss at step 58000: 3.234410\n",
      "Average loss at step 60000: 3.279726\n",
      "Nearest to 's: his, the, ich, unas, considers, coeducational, and, exports,\n",
      "Nearest to and: ,, ., or, but, that, UNK, cursed, the,\n",
      "Nearest to one: only, steadily, each, three, grandis, 's, it, the,\n",
      "Nearest to be: was, have, is, belts, burma, are, them, but,\n",
      "Nearest to also: creation-science, this, it, that, aires, selassie, although, nullification,\n",
      "Nearest to ): UNK, ,, (, '', in, or, bakersfield, sinography,\n",
      "Nearest to was: is, were, be, are, had, has, in, dells,\n",
      "Nearest to from: into, in, crookes, bardolatry, tann, studios, where, due,\n",
      "Nearest to by: that, in, into, jiā, scope=, these, infamously, .,\n",
      "Nearest to which: but, or, 20nm, where, pertinent, that, sh, and,\n",
      "Nearest to were: are, was, had, other, can, all, appear, they,\n",
      "Nearest to but: though, which, because, and, that, not, can, although,\n",
      "Nearest to other: are, they, these, some, all, were, many, small,\n",
      "Nearest to this: it, the, its, an, a, also, however, no,\n",
      "Nearest to he: his, it, undefended, gta, they, top-to-bottom, she, had,\n",
      "Nearest to to: would, could, can, them, must, that, visconti, nudum,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 62000: 3.248929\n",
      "Average loss at step 64000: 3.266676\n",
      "Average loss at step 66000: 3.253489\n",
      "Average loss at step 68000: 3.285432\n",
      "Average loss at step 70000: 3.239971\n",
      "Nearest to 's: his, the, first, objectives, one, valueless, in, of,\n",
      "Nearest to and: ,, ., the, in, UNK, (, of, ching,\n",
      "Nearest to one: most, two, three, part, only, the, 's, mva,\n",
      "Nearest to be: jazzy, utilised, have, technician, aft, burma, that, pending,\n",
      "Nearest to also: often, bethlen, 1569, quartered, hoppe, it, slowest, chowtal,\n",
      "Nearest to ): UNK, (, ,, madrigal, grunts, august, 1486, 2,\n",
      "Nearest to was: were, is, had, became, has, ,, garry, racism,\n",
      "Nearest to from: sassanid, burke, o'neill, marauders, in, classical, to, constructions,\n",
      "Nearest to by: who, and, mieszko, ginseng, ., the, myspace, thunder,\n",
      "Nearest to which: a, antti, committing, giorgos, pages, meantime, arbitrator, courtyards,\n",
      "Nearest to were: are, was, had, enlistment, outspoken, two, non-theistic, bluffs,\n",
      "Nearest to but: though, however, because, so, edvac, annihilation, still, that,\n",
      "Nearest to other: many, such, these, are, spike, items, quips, kane,\n",
      "Nearest to this: it, its, however, familiar, an, he, sabre, skłodowska-curie,\n",
      "Nearest to he: it, she, him, that, never, his, gta, this,\n",
      "Nearest to to: absolutely, song, towards, 48,000, advancement, xor, locative, from,\n",
      "Average loss at step 72000: 3.266263\n",
      "Average loss at step 74000: 3.254907\n",
      "Average loss at step 76000: 3.199326\n",
      "Average loss at step 78000: 3.229158\n",
      "Average loss at step 80000: 3.212704\n",
      "Nearest to 's: his, and, the, considers, a, of, 1521, disambiguate,\n",
      "Nearest to and: with, ., ,, in, to, 's, including, of,\n",
      "Nearest to one: three, doorkeeper, biosciences, descriptive, disgruntled, spartans, print, investigate,\n",
      "Nearest to be: have, were, quarantine, they, polished, detects, give, hirohito,\n",
      "Nearest to also: been, odessa, no, bethlen, that, by, inflates, facilitation,\n",
      "Nearest to ): UNK, (, 4, 16, km, 24, 2, million,\n",
      "Nearest to was: is, became, ,, had, are, muscovites, rolf, him,\n",
      "Nearest to from: between, through, in, dip, starling, karoo, enquiries, dit,\n",
      "Nearest to by: interdependence, 1572., also, unmeasured, and, forlorn, which, ender,\n",
      "Nearest to which: that, who, although, i̇stanbul, mua, belatedly, 1054, but,\n",
      "Nearest to were: are, be, detect, all, 1709, .85, anglo-french, menu,\n",
      "Nearest to but: though, so, not, because, although, however, they, even,\n",
      "Nearest to other: these, many, several, those, kane, all, different, both,\n",
      "Nearest to this: it, however, its, the, an, another, 219, monaghan,\n",
      "Nearest to he: him, later, his, sinemurian, it, paloma, they, struggling,\n",
      "Nearest to to: and, sediment, ,, for, contradicted, mussolini, single-family, meshkov,\n",
      "Average loss at step 82000: 3.238996\n",
      "Average loss at step 84000: 3.240081\n",
      "Average loss at step 86000: 3.222651\n",
      "Average loss at step 88000: 3.187617\n",
      "Average loss at step 90000: 3.229360\n",
      "Nearest to 's: its, one, state, national, calculations, largest, immortality, the,\n",
      "Nearest to and: including, ,, in, with, of, laptop, 's, houses,\n",
      "Nearest to one: three, 's, most, the, largest, two, each, ogdoad,\n",
      "Nearest to be: have, move, tuphōn, pentatonic, make, polished, were, one-stringed,\n",
      "Nearest to also: some, as, mackay, when, into, main, been, many,\n",
      "Nearest to ): UNK, m, see, hudson, conquerors, (, re-engineered, 24,\n",
      "Nearest to was: is, were, examining, being, are, tlaloc, has, tigris,\n",
      "Nearest to from: into, between, within, cycads, up, goddess, accusers, tongarewa,\n",
      "Nearest to by: waive, discipline, which, machinist, bombardments, within, from, gathered,\n",
      "Nearest to which: that, but, when, barriers, where, while, frictionless, divided,\n",
      "Nearest to were: are, was, anglo-french, can, all, had, coulthard, rock-based,\n",
      "Nearest to but: though, although, while, which, however, because, annihilation, interruption,\n",
      "Nearest to other: these, some, many, several, different, are, three, various,\n",
      "Nearest to this: it, another, voracious, abstractly, sudanese, velázquez, its, cárdenas,\n",
      "Nearest to he: his, it, hijacking, later, him, roentgen, rath, never,\n",
      "Nearest to to: towards, could, suburbs, would, anubis, of, heartbeat, stallman,\n",
      "Average loss at step 92000: 3.261271\n",
      "Average loss at step 94000: 3.207644\n",
      "Average loss at step 96000: 3.218390\n",
      "Average loss at step 98000: 3.191866\n",
      "Average loss at step 100000: 3.263311\n",
      "Nearest to 's: his, schumpeter, its, the, hand-over, her, itinerant, hirst,\n",
      "Nearest to and: with, ,, UNK, ., in, the, of, to,\n",
      "Nearest to one: most, more, cody, sectors, sheared, people, important, any,\n",
      "Nearest to be: have, pentatonic, teaching, were, produce, are, refer, biochemist,\n",
      "Nearest to also: often, dance, which, it, by, indonesians, some, and,\n",
      "Nearest to ): UNK, ,, (, with, e.g, redirect, imitators, twinning,\n",
      "Nearest to was: is, became, were, digression, had, guarantee, núñez, are,\n",
      "Nearest to from: between, into, in, to, triforces, swimmers, appended, among,\n",
      "Nearest to by: chakravarti, gathered, also, 1671, and, intuitively, with, among,\n",
      "Nearest to which: that, but, considered, also, who, where, ., and,\n",
      "Nearest to were: are, had, was, all, other, be, bluffs, have,\n",
      "Nearest to but: if, though, although, considered, which, because, still, not,\n",
      "Nearest to other: these, many, are, all, some, several, their, different,\n",
      "Nearest to this: it, another, a, crozier, chlamydomonas, transmigrates, 123, pacing,\n",
      "Nearest to he: it, she, himself, gta, eli, intends, indo-pakistani, decisively,\n",
      "Nearest to to: ., for, and, zion, of, with, the, unannounced,\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class config_sg:\n",
    "    \"\"\"\"\"\"\n",
    "    batch_size = 128\n",
    "    embedding_size = 128\n",
    "    window_size = 4\n",
    "    vocabulary_size = 50000\n",
    "\n",
    "    # A random validation set\n",
    "    valid_size = 16\n",
    "    valid_window = 50\n",
    "\n",
    "    num_sampled = 32  # 负采样\n",
    "\n",
    "    num_steps = 100001\n",
    "    top_k = 8\n",
    "    \n",
    "config = config_sg\n",
    "\n",
    "valid_window, valid_size = config.valid_window, config.valid_size\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_examples = np.append(valid_examples, random.sample(range(1000, 1000 + valid_window), valid_size), axis=0)\n",
    "\n",
    "# Graph\n",
    "tf.reset_default_graph()\n",
    "g = tf.get_default_graph()\n",
    "\n",
    "# Input\n",
    "batch_size = config.batch_size\n",
    "train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "# Word2Vec Model\n",
    "# Embedding layer\n",
    "vocabulary_size, embedding_size = config.vocabulary_size, config.embedding_size\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "\n",
    "# Softmax Weights and Biases\n",
    "softmax_W = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                        stddev=0.5 / math.sqrt(embedding_size)))\n",
    "softmax_b = tf.Variable(tf.random_uniform([vocabulary_size], 0.0, 0.01))\n",
    "\n",
    "# 负采样\n",
    "num_sampled = config.num_sampled\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(\n",
    "        weights=softmax_W, biases=softmax_b, inputs=embed,\n",
    "        labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "# train_op\n",
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "\n",
    "# 计算相似度\n",
    "# norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "# normalized_embeddings = embeddings / norm\n",
    "normalized_embeddings = tf.nn.l2_normalize(embeddings, axis=1)  # 等价于以上两行\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "# 计算 cosine 相似度（内积）\n",
    "similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
    "\n",
    "# 准备数据\n",
    "# filename = \"./data/wikipedia2text-extracted.txt.bz2\"  # for Ipython\n",
    "# # words = read_data(filename)\n",
    "# words = read_data_small(filename)\n",
    "# print('Data size %d' % len(words))\n",
    "# print('Example words (start): ', words[:10])\n",
    "# print('Example words (end): ', words[-10:])\n",
    "\n",
    "# data, count, word2id, id2word = build_dataset(words)\n",
    "# print('Most common words (+UNK)', count[:5])\n",
    "# print('Sample data', data[:10])\n",
    "# del words\n",
    "\n",
    "# Train\n",
    "skip_losses = []\n",
    "num_steps = config.num_steps\n",
    "with tf.Session(graph=g) as sess:\n",
    "    \"\"\"\"\"\"\n",
    "    # init\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        batch_data, batch_labels = generate_batch_sg(data, batch_size, config.window_size)\n",
    "\n",
    "        # run train_op and get loss\n",
    "        _, loss_val = sess.run([optimizer, loss], feed_dict={train_dataset: batch_data,\n",
    "                                                             train_labels: batch_labels})\n",
    "\n",
    "        # Update the average loss\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if (step + 1) % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "\n",
    "            skip_losses.append(average_loss)\n",
    "            print('Average loss at step %d: %f' % (step + 1, average_loss))\n",
    "            average_loss = 0\n",
    "\n",
    "        # 评价\n",
    "        if (step + 1) % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = id2word[valid_examples[i]]\n",
    "                top_k = config.top_k\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = id2word[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "\n",
    "    embeddings_sg = normalized_embeddings.eval()\n",
    "\n",
    "# We will save the word vectors learned and the loss over time\n",
    "# as this information is required later for comparisons\n",
    "np.save('./out/embeddings_sg', embeddings_sg)\n",
    "\n",
    "with open('skip_losses.csv', 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(skip_losses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
